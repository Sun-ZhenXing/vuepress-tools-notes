import{_ as i,r as c,o as r,c as u,b as n,d as s,a,w as e,e as o}from"./app-bd1d9ffa.js";const d={},m=n("h1",{id:"wsl-2-中搭建深度学习环境",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#wsl-2-中搭建深度学习环境","aria-hidden":"true"},"#"),s(" WSL 2 中搭建深度学习环境")],-1),k={class:"table-of-contents"},b=o('<h2 id="_1-wsl-2-进行深度学习的最佳实践" tabindex="-1"><a class="header-anchor" href="#_1-wsl-2-进行深度学习的最佳实践" aria-hidden="true">#</a> 1. WSL 2 进行深度学习的最佳实践</h2><p>【Q】为什么使用 Docker 进行深度学习？</p><p>【A】WSL 2 Docker 内执行速度大约为 Ubuntu 主机的 80%，带来了一些性能牺牲。但是 Docker 的优势远比这点损失来的多：</p><ul><li>随时启动和停止一个环境</li><li>环境与主机隔离，主机可以正在做别的事情</li><li>同时运行多个环境，并分配 GPU</li><li>随意切换 CUDA 版本</li><li>随时备份和恢复一个环境，可将镜像迁移到不同机器上运行</li></ul><p>【Q】为什么使用 WSL 2 进行深度学习？</p><p>【A】可以同时使用 Linux 的训练环境和 Windows 的便捷界面，而且互不影响，可以协同工作。</p><p>【Q】如果我的数据集较大（或者在外置磁盘中），应该如何操作？</p><p>【A】创建容器时使用 <code>-v host_path:container_path</code> 挂载路径，Windows 和 Docker 容器可共享此路径，这样可以直接在 Windows 下操作文件，然后在容器内训练，建议所有深度学习的容器都挂载同一个位置，方便共享数据。详细操作见下文。（注意：这不适合大数据集，因为 WSL 2 的跨文件系统 I/O 性能较差，如果希望在 WSL 2 中使用高性能的 I/O，建议复制到 Linux 内）。</p><p>【Q】如果我想使用 TensorBoard 或者 Jupyter 怎么办？</p><p>【A】映射端口即可，见下文。如果你希望使用 Matplotlib 等绘图工具，建议在 Jupyter 中进行。</p><p>【Q】如果我希望使用 IDE 在容器内开发程序，并进行调试，应该怎么做？</p>',11),h=n("p",null,"【Q】我在创建容器之后想修改容器的配置，如增加挂载和映射端口应该怎么做？",-1),v=o(`<p>【Q】如果我想快速存取文件，例如取出权重文件，或指定测试文件，但是这个路径不在共享路径下怎么办？</p><p>【A】使用 <code>docker cp</code> 复制文件，可以从主机复制到容器，也可以从容器复制到主机。</p><p>还可以开启 HTTP 服务或者 FTP 服务，可以互相访问内容。容器可以直接读取主机监听的端口，从而可以直接 <code>wget</code> 下载主机的文件。开启 HTTP 服务：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python <span class="token parameter variable">-m</span> http.server <span class="token number">8000</span>
</code></pre></div><p>也有许多的第三方软件，可在不同环境共享文件。</p><p>【Q】如果需要不同的依赖环境，有哪些做法？</p><p>【A】大致可有下面两种做法：</p><ol><li>在不同的容器内进行开发，PyTorch 拉取 PyTorch 镜像，TensorFlow 拉取 TensorFlow 镜像，可以拉取各种不同版本的镜像，单独来开发，环境全部相互隔离。缺点是占用空间较大，不过这点空间和训练集相比可以忽略。</li><li>在同一个基础容器（指 CUDA 容器，普通容器不行）内使用 Miniconda 创建虚拟环境开发，好处是操作简单，占用小。缺点是无法隔离 CUDA、cuDNN 等环境，不过影响不大，因为现代框架支持性较好，可提供不同版本的框架。</li></ol><p>各种不同的镜像拉取示例：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> pull tensorflow/tensorflow:2.11.0
<span class="token function">docker</span> pull pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime
<span class="token function">docker</span> pull nvidia/cuda:11.8.0-base-ubuntu22.04
</code></pre></div><p>如果本地网络较差，可使用代理拉取，也可以配置 Docker 镜像，或者在云端拉取然后将打包回传到本地。</p><p>【Q】Docker 没有 GUI，因此无法使用 <code>cv2.imshow</code>，有时还不能导入 <code>cv2</code>，怎么解决？</p><p>无法导入 OpenCV 时，确保 OpenCV 的安装顺序，或者只安装有 <code>-headless</code> 后缀的版本。</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> opencv-python
pip3 <span class="token function">install</span> opencv-contrib-python
pip3 <span class="token function">install</span> opencv-python-headless
pip3 <span class="token function">install</span> opencv-contrib-python-headless
</code></pre></div><p>无法显示图片，这个除非有图形界面，使用 <code>cv2.imwrite</code>，然后在 Windows 下查看即可。</p><p>【Q】如果希望备份整个开发环境，应该怎么办？</p><p>【A】Docker 可将容器导出为镜像，镜像可以随时备份为文件，可以迁移到其他电脑或其他任何环境。使用 <code>docker commit</code> 可导出容器为镜像，<code>docker save</code> 可将镜像压缩为一个文件。还可以使用 Docker Hub 共享镜像到社区。</p>`,17),g=n("code",null,"wsl --export",-1),f=o(`<h2 id="_2-条件准备" tabindex="-1"><a class="header-anchor" href="#_2-条件准备" aria-hidden="true">#</a> 2. 条件准备</h2><ul><li>主机是现代 CPU 且是 x86 架构，安装有现代的 NVIDIA 显卡</li><li>需要 Windows 10 以上并安装有 WSL 2。如果不了解如何安装可参考网络</li><li>首先需要安装 Docker Desktop，这同时会安装 WSL 2 的两个容器 <code>docker-desktop-data</code> 和 <code>docker-desktop</code></li></ul><h2 id="_3-如何使用" tabindex="-1"><a class="header-anchor" href="#_3-如何使用" aria-hidden="true">#</a> 3. 如何使用</h2><p>现在我们在主机查看 NVIDIA 显卡信息：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>nvidia-smi
</code></pre></div><p>得到 CUDA 版本，这个版本是你可以安装 CUDA 的最高版本，CUDA 是向下兼容的，因此可以使用比这更小的版本。</p>`,6),_={class:"hint-container info"},y=n("p",{class:"hint-container-title"},"升级显卡驱动",-1),x={href:"https://pytorch.org/blog/deprecation-cuda-python-support/",target:"_blank",rel:"noopener noreferrer"},D=o(`<p>其次：</p><ul><li><strong>不需要</strong> 在 Windows 上安装 CUDA 驱动</li><li><strong>不需要</strong> 在 Windows 上安装 cuDNN 组件</li><li><strong>不需要</strong> 在 WSL 2 内安装显卡驱动或其他</li><li><strong>不需要</strong> 在容器内安装 CUDA 或其他</li></ul><p>这就是全部了，如果上述条件都满足就可以使用 PyTorch、TensorFlow 或任何你需要的环境继续了？</p><p>这是因为 WSL 2 内核支持的 Docker 已经支持 <code>--gpus</code> 了（Docker 版本大于 19.03 即可），再也不需要 <code>nvidia-docker2</code> 来工作了。</p><h2 id="_4-安装-cuda-容器" tabindex="-1"><a class="header-anchor" href="#_4-安装-cuda-容器" aria-hidden="true">#</a> 4. 安装 CUDA 容器</h2><p>拉取 CUDA 11.6 Ubuntu 20.04 镜像（也可以直接拉取 PyTorch 镜像）：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> pull nvidia/cuda:11.6.0-base-ubuntu20.04
</code></pre></div><p>创建容器，可指定参数：</p><ul><li>如果需要读外部数据请挂载目录或磁盘，如 <code>-v /mnt/d/docker_shared:/shared_data</code>（WSL 2 内的磁盘路径为 <code>/mnt/c</code>、<code>/mnt/d</code>，对应 C、D 盘）</li><li>如果需要使用 Jupyter Notebook 请映射端口，如 <code>-p 8888:8888</code></li><li>如果机器有多个显卡，可以指定其序号 <code>--gpus 1,3</code>，一块 GPU 只能分配到一个正在运行的容器内</li></ul><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-itd</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span> ub-cu11.6 <span class="token punctuation">\\</span>
    nvidia/cuda:11.6.0-base-ubuntu20.04 /bin/bash
</code></pre></div><p>创建一个更复杂的容器：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-itd</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /mnt/d/docker_shared:/shared_data <span class="token punctuation">\\</span>
    <span class="token parameter variable">-p</span> <span class="token number">8888</span>:8888 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span> ub-cu11.6 <span class="token punctuation">\\</span>
    nvidia/cuda:11.6.0-base-ubuntu20.04 /bin/bash
</code></pre></div><p>进入容器：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> ub-cu11.6 /bin/bash
</code></pre></div><p>现在和 Linux 系统一致了。</p><h2 id="_5-在容器内安装深度学习环境" tabindex="-1"><a class="header-anchor" href="#_5-在容器内安装深度学习环境" aria-hidden="true">#</a> 5. 在容器内安装深度学习环境</h2><p>更新镜像源（以阿里云为例）：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">mv</span> /etc/apt/sources.list /etc/apt/sources.list-bak
<span class="token builtin class-name">echo</span> <span class="token string">&#39;deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&#39;</span> <span class="token operator">&gt;</span> /etc/apt/sources.list
</code></pre></div><p>配置镜像后，下列操作基本为满速，十几分钟即可安装完成全部依赖。</p><p>安装工具：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">apt</span> update
<span class="token function">apt</span> upgrade <span class="token parameter variable">-y</span>
<span class="token function">apt</span> <span class="token function">install</span> <span class="token function">sudo</span> <span class="token function">vim</span> <span class="token function">wget</span> <span class="token function">curl</span> <span class="token function">git</span> <span class="token function">zip</span> <span class="token function">unzip</span> <span class="token function">tar</span> <span class="token parameter variable">-y</span>

<span class="token function">apt</span> <span class="token function">install</span> python3-pip
</code></pre></div><p>（可选）添加用户，并将用户可执行文件加入 PATH：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">useradd</span> <span class="token parameter variable">-r</span> <span class="token parameter variable">-m</span> <span class="token parameter variable">-s</span> /bin/bash admin
<span class="token comment"># 如果提示输入密码则输入密码</span>
<span class="token comment"># 如果没有提示则使用下面的命令修改密码</span>
<span class="token function">passwd</span> admin

<span class="token function">su</span> admin
<span class="token builtin class-name">cd</span>
<span class="token builtin class-name">echo</span> <span class="token string">&#39;export PATH=&quot;/home/admin/.local/bin:$PATH&quot;&#39;</span> <span class="token operator">&gt;</span> ~/.bashrc
<span class="token builtin class-name">source</span> ~/.bashrc
</code></pre></div><p>安装 Python 环境和常见依赖：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple <span class="token parameter variable">--upgrade</span> pip

<span class="token comment"># 配置负载均衡的 PyPI 镜像，可快速选择较快的镜像源</span>
pip3 config <span class="token builtin class-name">set</span> global.extra-index-url <span class="token string">&quot;https://pypi.tuna.tsinghua.edu.cn/simple/ https://mirrors.aliyun.com/pypi/simple/ https://repo.huaweicloud.com/repository/pypi/simple/ https://mirrors.bfsu.edu.cn/pypi/web/simple/&quot;</span>
pip3 <span class="token function">install</span> opencv-python
pip3 <span class="token function">install</span> opencv-contrib-python
pip3 <span class="token function">install</span> opencv-python-headless
pip3 <span class="token function">install</span> opencv-contrib-python-headless
</code></pre></div><p>安装 PyTorch：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
</code></pre></div><p>（可选）安装 YOLOv8 环境：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> ultralytics
</code></pre></div><p>测试 YOLOv8 在 3070Ti 上的速度：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>admin@f292841cc5bf:~$ yolo predict <span class="token assign-left variable">model</span><span class="token operator">=</span>yolov8x.pt <span class="token assign-left variable">source</span><span class="token operator">=</span><span class="token string">&quot;bus.jpg&quot;</span>
Ultralytics YOLOv8.0.34 🚀 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 <span class="token punctuation">(</span>NVIDIA GeForce RTX <span class="token number">3070</span> Ti Laptop GPU, 8192MiB<span class="token punctuation">)</span>
YOLOv8x summary <span class="token punctuation">(</span>fused<span class="token punctuation">)</span>: <span class="token number">268</span> layers, <span class="token number">68200608</span> parameters, <span class="token number">0</span> gradients, <span class="token number">257.8</span> GFLOPs

image <span class="token number">1</span>/1 /home/admin/bus.jpg: 640x480 <span class="token number">5</span> persons, <span class="token number">1</span> bicycle, <span class="token number">1</span> bus, <span class="token number">32</span>.4ms
Speed: <span class="token number">9</span>.3ms pre-process, <span class="token number">32</span>.4ms inference, <span class="token number">2</span>.3ms postprocess per image at shape <span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">3</span>, <span class="token number">640</span>, <span class="token number">640</span><span class="token punctuation">)</span>
admin@f292841cc5bf:~$ yolo predict <span class="token assign-left variable">model</span><span class="token operator">=</span>yolov8x.pt <span class="token assign-left variable">source</span><span class="token operator">=</span><span class="token string">&quot;bus.jpg&quot;</span>
Ultralytics YOLOv8.0.34 🚀 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 <span class="token punctuation">(</span>NVIDIA GeForce RTX <span class="token number">3070</span> Ti Laptop GPU, 8192MiB<span class="token punctuation">)</span>
YOLOv8x summary <span class="token punctuation">(</span>fused<span class="token punctuation">)</span>: <span class="token number">268</span> layers, <span class="token number">68200608</span> parameters, <span class="token number">0</span> gradients, <span class="token number">257.8</span> GFLOPs

image <span class="token number">1</span>/1 /home/admin/bus.jpg: 640x480 <span class="token number">5</span> persons, <span class="token number">1</span> bicycle, <span class="token number">1</span> bus, <span class="token number">32</span>.1ms
Speed: <span class="token number">2</span>.0ms pre-process, <span class="token number">32</span>.1ms inference, <span class="token number">2</span>.0ms postprocess per image at shape <span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">3</span>, <span class="token number">640</span>, <span class="token number">640</span><span class="token punctuation">)</span>
admin@f292841cc5bf:~$ yolo predict <span class="token assign-left variable">model</span><span class="token operator">=</span>yolov8s.pt <span class="token assign-left variable">source</span><span class="token operator">=</span><span class="token string">&quot;bus.jpg&quot;</span>
Ultralytics YOLOv8.0.34 🚀 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 <span class="token punctuation">(</span>NVIDIA GeForce RTX <span class="token number">3070</span> Ti Laptop GPU, 8192MiB<span class="token punctuation">)</span>
YOLOv8s summary <span class="token punctuation">(</span>fused<span class="token punctuation">)</span>: <span class="token number">168</span> layers, <span class="token number">11156544</span> parameters, <span class="token number">0</span> gradients, <span class="token number">28.6</span> GFLOPs

image <span class="token number">1</span>/1 /home/admin/bus.jpg: 640x480 <span class="token number">4</span> persons, <span class="token number">1</span> bus, <span class="token number">13</span>.0ms
Speed: <span class="token number">1</span>.9ms pre-process, <span class="token number">13</span>.0ms inference, <span class="token number">2</span>.3ms postprocess per image at shape <span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">3</span>, <span class="token number">640</span>, <span class="token number">640</span><span class="token punctuation">)</span>
admin@f292841cc5bf:~$ yolo predict <span class="token assign-left variable">model</span><span class="token operator">=</span>yolov8n.pt <span class="token assign-left variable">source</span><span class="token operator">=</span><span class="token string">&quot;bus.jpg&quot;</span>
Ultralytics YOLOv8.0.34 🚀 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 <span class="token punctuation">(</span>NVIDIA GeForce RTX <span class="token number">3070</span> Ti Laptop GPU, 8192MiB<span class="token punctuation">)</span>
YOLOv8n summary <span class="token punctuation">(</span>fused<span class="token punctuation">)</span>: <span class="token number">168</span> layers, <span class="token number">3151904</span> parameters, <span class="token number">0</span> gradients, <span class="token number">8.7</span> GFLOPs

image <span class="token number">1</span>/1 /home/admin/bus.jpg: 640x480 <span class="token number">4</span> persons, <span class="token number">1</span> bus, <span class="token number">1</span> stop sign, <span class="token number">10</span>.5ms
Speed: <span class="token number">2</span>.3ms pre-process, <span class="token number">10</span>.5ms inference, <span class="token number">2</span>.1ms postprocess per image at shape <span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">3</span>, <span class="token number">640</span>, <span class="token number">640</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,31);function L(w,A){const t=c("RouterLink"),p=c("router-link"),l=c("ExternalLinkIcon");return r(),u("div",null,[m,n("p",null,[s("在 "),a(t,{to:"/docker/projects/deeplearning-env.html"},{default:e(()=>[s("Docker 搭建深度学习环境")]),_:1}),s(" 一文中，我们使用 Ubuntu 搭建了 Docker 下的深度学习环境。而 WSL 2 同样也提供了完整的 CUDA 支持，我们可以在 Windows 下享受 Linux 环境下的优势。")]),n("nav",k,[n("ul",null,[n("li",null,[a(p,{to:"#_1-wsl-2-进行深度学习的最佳实践"},{default:e(()=>[s("1. WSL 2 进行深度学习的最佳实践")]),_:1})]),n("li",null,[a(p,{to:"#_2-条件准备"},{default:e(()=>[s("2. 条件准备")]),_:1})]),n("li",null,[a(p,{to:"#_3-如何使用"},{default:e(()=>[s("3. 如何使用")]),_:1})]),n("li",null,[a(p,{to:"#_4-安装-cuda-容器"},{default:e(()=>[s("4. 安装 CUDA 容器")]),_:1})]),n("li",null,[a(p,{to:"#_5-在容器内安装深度学习环境"},{default:e(()=>[s("5. 在容器内安装深度学习环境")]),_:1})])])]),b,n("p",null,[s("【A】例如使用 VS Code 开发，需要安装 Docker 和远程开发插件。请参考 "),a(t,{to:"/docker/projects/vscode-use-docker.html"},{default:e(()=>[s("VS Code 使用 Docker")]),_:1}),s("。同样的 PyCharm 和其他 JetBrain 系列 IDE 也支持容器内开发。推荐使用挂载到 Windows 下的路径进行开发，然后使用远程开发能力，在 Windows 下实时预览生成结果。")]),h,n("p",null,[s("【A】请参考 "),a(t,{to:"/docker/wsl/wsl-docker-config.html"},{default:e(()=>[s("基于 WSL 2 的 Docker 配置说明")]),_:1}),s("。")]),v,n("p",null,[s("如果你希望把整个 WSL 2 都备份了，可以使用 "),g,s(" 来导出为一个文件，详情见 "),a(t,{to:"/docker/wsl/migrate-docker-location.html"},{default:e(()=>[s("迁移 Docker 的位置")]),_:1}),s("。")]),f,n("div",_,[y,n("p",null,[s("如果你的 CUDA 版本小于 11.6 就建议升级显卡驱动了，PyTorch 即将取消 Python 3.7 和 CUDA 11.6 的支持（自 2023/2/1 开始，见 "),n("a",x,[s("官方博客"),a(l)]),s("），显卡驱动安装最新版通常不会出问题。")])]),D])}const P=i(d,[["render",L],["__file","wsl-deeplearning-env.html.vue"]]);export{P as default};
