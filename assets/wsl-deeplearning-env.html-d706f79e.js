import{_ as i,M as o,p as r,q as d,R as n,t as a,N as s,V as e,a1 as p}from"./framework-8980b429.js";const u={},h=n("h1",{id:"wsl2-中搭建深度学习环境",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#wsl2-中搭建深度学习环境","aria-hidden":"true"},"#"),a(" WSL2 中搭建深度学习环境")],-1),k={class:"table-of-contents"},b=p(`<h2 id="_1-wsl2-进行深度学习的最佳实践" tabindex="-1"><a class="header-anchor" href="#_1-wsl2-进行深度学习的最佳实践" aria-hidden="true">#</a> 1. WSL2 进行深度学习的最佳实践</h2><p>【Q】为什么使用 Docker 进行深度学习？</p><p>【A】WSL2 Docker 内执行速度大约为 Ubuntu 主机的 80%，带来了一些性能牺牲。但是 Docker 的优势远比这点损失来的多：</p><ul><li>随时启动和停止一个环境</li><li>环境与主机隔离，主机可以正在做别的事情</li><li>同时运行多个环境，并分配 GPU</li><li>随意切换 CUDA 版本</li><li>随时备份和恢复一个环境，可将镜像迁移到不同机器上运行</li></ul><p>【Q】为什么使用 WSL2 进行深度学习？</p><p>【A】可以同时使用 Linux 的训练环境和 Windows 的便捷界面，而且互不影响，可以协同工作。</p><p>【Q】如果我的数据集较大（或者在外置磁盘中），应该如何操作？</p><p>【A】创建容器时使用 <code>-v host_path:container_path</code> 挂载路径，Windows 和 Docker 容器可共享此路径，这样可以直接在 Windows 下操作文件，然后在容器内训练，建议所有深度学习的容器都挂载同一个位置，方便共享数据。详细操作见下文。</p><p>【Q】如果我想使用 <code>tensorboard</code> 或者 <code>jupyter</code> 怎么办？</p><p>【A】映射端口即可。见下文。</p><p>【Q】如果我想快速存取文件，例如取出权重文件，或指定测试文件，但是这个路径不在共享路径下怎么办？</p><p>【A】使用 <code>docker cp</code> 复制文件，可以从主机复制到容器，也可以从容器复制到主机。</p><p>还可以开启 HTTP 服务或者 FTP 服务，可以互相访问内容。容器可以直接读取主机监听的端口，从而可以直接 <code>wget</code> 下载主机的文件。开启 HTTP 服务：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> http.server <span class="token number">8000</span>
</code></pre></div><p>也有许多的第三方软件，可在不同环境共享文件。</p><p>【Q】如果需要不同的依赖环境，有哪些做法？</p><p>【A】大致可有下面两种做法：</p><ol><li>在不同的容器内进行开发，PyTorch 拉取 PyTorch 镜像，TensorFlow 拉取 TensorFlow 镜像，可以拉取各种不同版本的镜像，单独来开发，环境全部相互隔离。缺点是占用空间较大，不过这点空间和训练集相比可以忽略。</li><li>在同一个基础容器（指 CUDA 容器，普通容器不行）内使用 Miniconda 创建虚拟环境开发，好处是操作简单，占用小。缺点是无法隔离 CUDA、cuDNN 等环境，不过影响不大，因为现代框架支持性较好，可提供不同版本的框架。</li></ol><p>各种不同的镜像拉取示例：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> pull tensorflow/tensorflow:2.11.0
<span class="token function">docker</span> pull pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime
<span class="token function">docker</span> pull nvidia/cuda:11.8.0-base-ubuntu22.04
</code></pre></div><p>如果本地网络较差，可使用代理拉取，也可以配置 Docker 镜像，或者在云端拉取然后将打包回传到本地。</p><p>【Q】Docker 没有 GUI，因此无法使用 <code>cv2.imshow</code>，有时还不能导入 <code>cv2</code>，怎么解决？</p><p>无法导入 OpenCV 时，确保 OpenCV 的安装顺序，或者只安装有 <code>-headless</code> 后缀的版本。</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> opencv-python
pip3 <span class="token function">install</span> opencv-contrib-python
pip3 <span class="token function">install</span> opencv-python-headless
pip3 <span class="token function">install</span> opencv-contrib-python-headless
</code></pre></div><p>无法显示图片，这个除非有图形界面，使用 <code>cv2.imwrite</code>，然后在 Windows 下查看即可。</p><p>【Q】如果希望备份整个开发环境，应该怎么办？</p><p>【A】Docker 可将容器导出为镜像，镜像可以随时备份为文件，可以迁移到其他电脑或其他任何环境。使用 <code>docker commit</code> 可导出容器为镜像，<code>docker save</code> 可将镜像压缩为一个文件。还可以使用 Docker Hub 共享镜像到社区。</p>`,27),m=n("code",null,"wsl --export",-1),v=p(`<h2 id="_2-条件准备" tabindex="-1"><a class="header-anchor" href="#_2-条件准备" aria-hidden="true">#</a> 2. 条件准备</h2><ul><li>主机是现代 CPU 且是 x86 架构，安装有现代的 NVIDIA 显卡</li><li>需要 Windows 10 以上并安装有 WSL2。如果不了解如何安装可参考网络</li><li>首先需要安装 Docker Desktop，这同时会安装 WSL2 的两个容器 <code>docker-desktop-data</code> 和 <code>docker-desktop</code></li></ul><h2 id="_3-如何使用" tabindex="-1"><a class="header-anchor" href="#_3-如何使用" aria-hidden="true">#</a> 3. 如何使用</h2><p>现在我们在主机查看 NVIDIA 显卡信息：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>nvidia-smi
</code></pre></div><p>得到 CUDA 版本，这个版本是你可以安装 CUDA 的最高版本，CUDA 是向下兼容的，因此可以使用比这更小的版本。</p>`,6),g={class:"hint-container info"},f=n("p",{class:"hint-container-title"},"升级显卡驱动",-1),_={href:"https://pytorch.org/blog/deprecation-cuda-python-support/",target:"_blank",rel:"noopener noreferrer"},y=p(`<p>其次：</p><ul><li><strong>不需要</strong> 在 Windows 上安装 CUDA 驱动</li><li><strong>不需要</strong> 在 Windows 上安装 cuDNN 组件</li><li><strong>不需要</strong> 在 WSL2 内安装显卡驱动或其他</li><li><strong>不需要</strong> 在容器内安装 CUDA 或其他</li></ul><p>这就是全部了，如果上述条件都满足就可以使用 PyTorch、TensorFlow 或任何你需要的环境继续了？</p><p>这是因为 WSL2 内核支持的 Docker 已经支持 <code>--gpus</code> 了（Docker 版本大于 19.03 即可），再也不需要 <code>nvidia-docker2</code> 来工作了。</p><h2 id="_4-安装-cuda-容器" tabindex="-1"><a class="header-anchor" href="#_4-安装-cuda-容器" aria-hidden="true">#</a> 4. 安装 CUDA 容器</h2><p>拉取 CUDA 11.6 Ubuntu 20.04 镜像（也可以直接拉取 PyTorch 镜像）：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> pull nvidia/cuda:11.6.0-base-ubuntu20.04
</code></pre></div><p>创建容器，可指定参数：</p><ul><li>如果需要读外部数据请挂载目录或磁盘，如 <code>-v /mnt/d/docker_shared:/shared_data</code>（WSL2 内的磁盘路径为 <code>/mnt/c</code>、<code>/mnt/d</code>，对应 C、D 盘）</li><li>如果需要使用 Jupyter Notebook 请映射端口，如 <code>-p 8888:8888</code></li><li>如果机器有多个显卡，可以指定其序号 <code>--gpus 1,3</code>，一块 GPU 只能分配到一个正在运行的容器内</li></ul><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-itd</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span> ub-cu11.6 <span class="token punctuation">\\</span>
    nvidia/cuda:11.6.0-base-ubuntu20.04 /bin/bash
</code></pre></div><p>创建一个更复杂的容器：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-itd</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /mnt/d/docker_shared:/shared_data <span class="token punctuation">\\</span>
    <span class="token parameter variable">-p</span> <span class="token number">8888</span>:8888 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span> ub-cu11.6 <span class="token punctuation">\\</span>
    nvidia/cuda:11.6.0-base-ubuntu20.04 /bin/bash
</code></pre></div><p>进入容器：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> ub-cu11.6 /bin/bash
</code></pre></div><p>现在和 Linux 系统一致了。</p><h2 id="_5-在容器内安装深度学习环境" tabindex="-1"><a class="header-anchor" href="#_5-在容器内安装深度学习环境" aria-hidden="true">#</a> 5. 在容器内安装深度学习环境</h2><p>更新镜像源：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">mv</span> /etc/apt/sources.list /etc/apt/sources.list-bak
<span class="token builtin class-name">echo</span> <span class="token string">&#39;deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&#39;</span> <span class="token operator">&gt;</span> /etc/apt/sources.list
</code></pre></div><p>配置镜像后，下列操作基本为满速，十几分钟即可安装完成全部依赖。</p><p>安装工具：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">apt</span> update
<span class="token function">apt</span> upgrade <span class="token parameter variable">-y</span>
<span class="token function">apt</span> <span class="token function">install</span> <span class="token function">sudo</span> <span class="token function">vim</span> <span class="token function">wget</span> <span class="token function">curl</span> <span class="token function">git</span> <span class="token function">zip</span> <span class="token function">unzip</span> <span class="token function">tar</span> <span class="token parameter variable">-y</span>

<span class="token function">apt</span> <span class="token function">install</span> python3-pip
</code></pre></div><p>（可选）添加用户，并将用户可执行文件加入 PATH：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code><span class="token function">useradd</span> <span class="token parameter variable">-r</span> <span class="token parameter variable">-m</span> <span class="token parameter variable">-s</span> /bin/bash admin
<span class="token comment"># 如果提示输入密码则输入密码</span>
<span class="token comment"># 如果没有提示则使用下面的命令修改密码</span>
<span class="token function">passwd</span> admin

<span class="token function">su</span> admin
<span class="token builtin class-name">cd</span>
<span class="token builtin class-name">echo</span> <span class="token string">&#39;export PATH=&quot;/home/admin/.local/bin:$PATH&quot;&#39;</span> <span class="token operator">&gt;</span> ~/.bashrc
<span class="token builtin class-name">source</span> ~/.bashrc
</code></pre></div><p>安装 Python 环境和常见依赖：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple <span class="token parameter variable">--upgrade</span> pip

<span class="token comment"># 配置负载均衡的 PyPI 镜像，可快速选择较快的镜像源</span>
pip3 config <span class="token builtin class-name">set</span> global.extra-index-url <span class="token string">&quot;https://pypi.tuna.tsinghua.edu.cn/simple/ https://mirrors.aliyun.com/pypi/simple/ https://repo.huaweicloud.com/repository/pypi/simple/ https://mirrors.bfsu.edu.cn/pypi/web/simple/&quot;</span>
pip3 <span class="token function">install</span> opencv-python
pip3 <span class="token function">install</span> opencv-contrib-python
pip3 <span class="token function">install</span> opencv-python-headless
pip3 <span class="token function">install</span> opencv-contrib-python-headless
</code></pre></div><p>安装 PyTorch：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
</code></pre></div><p>（可选）安装 YOLOv8 环境：</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> ultralytics
pip3 uninstall opencv-python
pip3 uninstall opencv-python-headless
pip3 <span class="token function">install</span> opencv-python-headless
</code></pre></div>`,29);function x(D,w){const c=o("RouterLink"),t=o("router-link"),l=o("ExternalLinkIcon");return r(),d("div",null,[h,n("p",null,[a("在 "),s(c,{to:"/docker/projects/deeplearning-env.html"},{default:e(()=>[a("Docker 搭建深度学习环境")]),_:1}),a(" 一文中，我们使用 Ubuntu 搭建了 Docker 下的深度学习环境。而 WSL2 同样也提供了完整的 CUDA 支持，我们可以在 Windows 下享受 Linux 环境下的优势。")]),n("nav",k,[n("ul",null,[n("li",null,[s(t,{to:"#_1-wsl2-进行深度学习的最佳实践"},{default:e(()=>[a("1. WSL2 进行深度学习的最佳实践")]),_:1})]),n("li",null,[s(t,{to:"#_2-条件准备"},{default:e(()=>[a("2. 条件准备")]),_:1})]),n("li",null,[s(t,{to:"#_3-如何使用"},{default:e(()=>[a("3. 如何使用")]),_:1})]),n("li",null,[s(t,{to:"#_4-安装-cuda-容器"},{default:e(()=>[a("4. 安装 CUDA 容器")]),_:1})]),n("li",null,[s(t,{to:"#_5-在容器内安装深度学习环境"},{default:e(()=>[a("5. 在容器内安装深度学习环境")]),_:1})])])]),b,n("p",null,[a("如果你希望把整个 WSL2 都备份了，可以使用 "),m,a(" 来导出为一个文件，详情见 "),s(c,{to:"/docker/wsl/migrate-docker-location.html"},{default:e(()=>[a("迁移 Docker 的位置")]),_:1}),a("。")]),v,n("div",g,[f,n("p",null,[a("如果你的 CUDA 版本小于 11.6 就建议升级显卡驱动了，PyTorch 即将取消 Python 3.7 和 CUDA 11.6 的支持（自 2023/2/1 开始，见 "),n("a",_,[a("官方博客"),s(l)]),a("），显卡驱动安装最新版通常不会出问题。")])]),y])}const U=i(u,[["render",x],["__file","wsl-deeplearning-env.html.vue"]]);export{U as default};
